{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7eaf13",
   "metadata": {},
   "source": [
    "# Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27956736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pathlib\n",
    "import math\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, array_to_img\n",
    "\n",
    "#Augmentation des images\n",
    "import random\n",
    "from tensorflow.keras.layers import RandomZoom, RandomRotation, RandomContrast, Rescaling, Resizing, RandomBrightness\n",
    "\n",
    "#Importation pour le modèle ResNet50V2 et l'encodage des labels\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.utils import to_categorical, load_img\n",
    "\n",
    "# Importations pour la construiction du modèle\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import RandomGrayscale\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Importations pour évaluation des performances\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, recall_score\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importation de l'utilitaire d'image de keras.utils\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fe9cd7",
   "metadata": {},
   "source": [
    "# Mobile Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e23a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, confusion_matrix\n",
    "# === Paramètres ===\n",
    "base_dir = r\"C:\\Users\\romai\\OneDrive\\Tiedostot\\Projet COVID\\DATASET_V2V2\"\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'val')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "batch_size = 16\n",
    "img_size = (224, 224)\n",
    "epochs = 10\n",
    "# === Générateurs d'images ===\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=15,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2]\n",
    ")\n",
    "val_test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "print(\"Classes détectées :\", train_generator.class_indices)\n",
    "# === Poids de classes ===\n",
    "class_weights = {\n",
    "    0: 3.0,  # COVID\n",
    "    1: 1.0,\n",
    "    2: 1.0,\n",
    "    3: 1.0\n",
    "}\n",
    "# === Construction du modèle ===\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=\"mobilenet_3111.keras\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
    "    checkpoint_cb\n",
    "]\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "# === Évaluation\n",
    "best_model = load_model(\"mobilenet_3111.keras\")\n",
    "test_generator.reset()\n",
    "test_pred_probs = best_model.predict(test_generator)\n",
    "test_preds = np.argmax(test_pred_probs, axis=1)\n",
    "y_test_true = test_generator.classes\n",
    "class_names = list(test_generator.class_indices.keys())\n",
    "# === Résultats\n",
    "test_acc = accuracy_score(y_test_true, test_preds)\n",
    "test_recall = recall_score(y_test_true, test_preds, average='macro')\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Recall (macro): {test_recall:.4f}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test_true, test_preds, target_names=class_names))\n",
    "cm = confusion_matrix(y_test_true, test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=class_names, yticklabels=class_names, cmap=\"Greens\")\n",
    "plt.title(\"Matrice de confusion - Test\")\n",
    "plt.xlabel(\"Prédit\")\n",
    "plt.ylabel(\"Réel\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd1457",
   "metadata": {},
   "source": [
    "# EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83742db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins des dossiers\n",
    "\n",
    "DATA_DIR = r'C:\\Users\\Morvan\\Documents\\DATA\\projet_covid\\COVID-PROJET\\DATASETS\\DATASET V2V2  - équilibré -augmenté_split'\n",
    "train_dir = DATA_DIR + '/train'\n",
    "val_dir = DATA_DIR + '/val'\n",
    "test_dir = DATA_DIR + '/test'\n",
    "\n",
    "# Paramètres\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "# Chargement des datasets\n",
    "train_ds = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "classes_names = train_ds.class_names\n",
    "num_classes = len(classes_names)\n",
    "\n",
    "# Préparation des datasets (préchargement et préfetch)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Conversion N&B -> 3 canaux pour EfficientNet (qui attend des images en 3 canaux)\n",
    "def convert_grayscale_to_rgb(image, label):\n",
    "    image = tf.image.grayscale_to_rgb(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(convert_grayscale_to_rgb)\n",
    "val_ds = val_ds.map(convert_grayscale_to_rgb)\n",
    "test_ds = test_ds.map(convert_grayscale_to_rgb)\n",
    "\n",
    "# Construction du modèle EfficientNet\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "base_model.trainable = False  # on freeze le modèle pour commencer\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False  # on freeze les couches du modèle de base\n",
    "\n",
    "for layer in base_model.layers[-20:]:    \n",
    "    layer.trainable = True  # on défreeze les 3 dernières couches\n",
    "\n",
    "# Ajout de la tête de classification\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "model = models.Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Résumé du modèle\n",
    "model_summary = model.summary()\n",
    "print(model_summary)\n",
    "\n",
    "# Poids de classes (à adapter selon ton jeu de données ou préférences)\n",
    "# Exemple : favoriser la détection de la classe 0 et sous-pondérer la 3\n",
    "class_weights = {\n",
    "    0: 3.0,     # COVID\n",
    "    1: 1.0,     #Lung_Opacity\n",
    "    2: 1.0,     # Normal\n",
    "    3: 1.0      # Viral Pneumonia\n",
    "}\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "weight_code = ''\n",
    "for value in class_weights.values():\n",
    "    weight_code += str(int(value))  #mise en forme x111 pour l'enregistrement\n",
    "\n",
    "# Entraînement\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=20,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "model_name = f'efficientnet_{timestr}_{weight_code}'\n",
    "model_path=f'models/{model_name}'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# Affichage des courbes de performance\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Courbe Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Train Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Val Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Accuracy')\n",
    "\n",
    "# Courbe Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Train Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Val Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.savefig(f\"{model_path}/training_history.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1049428",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin d'accès des datasets\n",
    "TRAINING_DIR = r\"C:\\Users\\ambro\\Desktop\\Datasets\\DATASET V2V2  - balanced -augmented_split\\train\"\n",
    "VALIDATION_DIR = r\"C:\\Users\\ambro\\Desktop\\Datasets\\DATASET V2V2  - balanced -augmented_split\\val\"\n",
    "TEST_DIR = r\"C:\\Users\\ambro\\Desktop\\Datasets\\DATASET V2V2  - balanced -augmented_split\\test\"\n",
    "\n",
    "## Création utilitaire d'images _ training\n",
    "train_ds = image_dataset_from_directory(TRAINING_DIR, # Chemin vers le répertoire contenant les images\n",
    "                                        seed = 42, # Germe aléatoire\n",
    "                                        batch_size = 64, # Taille des lots d'itération\n",
    "                                        image_size = (224,224),\n",
    "                                        labels='inferred',\n",
    "                                        label_mode='categorical',\n",
    "                                        color_mode='grayscale'\n",
    "                                       )\n",
    "\n",
    "\n",
    "## Création utilitaire d'images _ validation\n",
    "val_ds = image_dataset_from_directory(VALIDATION_DIR, # Chemin vers le répertoire contenant les images\n",
    "                                        seed = 42, # Germe aléatoire\n",
    "                                        batch_size = 64, # Taille des lots d'itération\n",
    "                                        image_size = (224,224),\n",
    "                                        labels='inferred',\n",
    "                                        label_mode='categorical',\n",
    "                                        color_mode='grayscale'\n",
    "                                       )\n",
    "\n",
    "## Création utilitaire d'images _ test\n",
    "test_ds = image_dataset_from_directory(TEST_DIR, # Chemin vers le répertoire contenant les images\n",
    "                                        seed = 42, # Germe aléatoire\n",
    "                                        batch_size = 64, # Taille des lots d'itération\n",
    "                                        image_size = (224,224),\n",
    "                                        labels='inferred',\n",
    "                                        label_mode='categorical',\n",
    "                                        color_mode='grayscale'\n",
    "                                       )\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Préchargement et préfetch\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Conversion de \"Grayscale\" en 3 canaux pour ResNet50_V2\n",
    "def convert_grayscale_to_rgb(image, label):\n",
    "    image = tf.image.grayscale_to_rgb(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(convert_grayscale_to_rgb)\n",
    "val_ds = val_ds.map(convert_grayscale_to_rgb)\n",
    "test_ds = test_ds.map(convert_grayscale_to_rgb)\n",
    "\n",
    "\n",
    "## Instanciation des callbacks\n",
    "\n",
    "# a) Arrêt prématuré\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss',       # Métrique surveillée\n",
    "                               min_delta = 0.01,           # Changement minimum de la métrique surveillée\n",
    "                               patience = 5,               # Nombre d'epochs sans amélioration pour arrêt\n",
    "                               mode = 'min',               # L'entraînement s'arrête quand 'val_loss' cesse de décroître\n",
    "                               verbose = 1,                # Affichage de l'epoch d'arrêt\n",
    "                               restore_best_weights = True # Restauration des meilleurs poids (après arrêt)\n",
    "                              )\n",
    "\n",
    "\n",
    "## b) Learning_rate réduit\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor = 'val_loss',       # Métrique surveillée\n",
    "                                         min_delta = 0.01,           # Changement minimum de la métrique surveillée\n",
    "                                         patience = 3,               # Nombre d'epochs sans amélioration pour arrêt\n",
    "                                         factor = 0.1,               # Learning_rate divisé par 10 (multiplié par 0.1)\n",
    "                                         cooldown = 4,               # Pause de 4 epochs entre 2 cycles\n",
    "                                         verbose = 1,                # Affichage de l'epoch d'arrêt\n",
    "                                         restore_best_weights = True # Restauration des meilleurs poids (après arrêt)\n",
    "                                        )\n",
    "\n",
    "\n",
    "## c) Sauvegarde auto du modèle durant l'entraînement\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "save0 = ModelCheckpoint(\"resnet_model_2V2_1111_20epochs.keras\",\n",
    "                       save_best_only=True, # Only saves when the model is considered the \"best\"\n",
    "                       monitor='val_loss', # The metric name to monitor\n",
    "                       mode='min' # Overwrite elder model when 'val_loss' is min\n",
    "                      )\n",
    "save1 = ModelCheckpoint(\"resnet_model_2V2_1111_20epochs.h5\",\n",
    "                       save_best_only=True, # Only saves when the model is considered the \"best\"\n",
    "                       monitor='val_loss', # The metric name to monitor\n",
    "                       mode='min' # Overwrite elder model when 'val_loss' is min\n",
    "                      )\n",
    "\n",
    "\n",
    "## Structure générale du modèle\n",
    "\n",
    "# Charger le modèle ResNet50V2 sans la partie supérieure\n",
    "base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# On \"freeze\" le modèle de base\n",
    "base_model.trainable = False\n",
    "\n",
    "# \"Freezer\" les couches du modèle de base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# \"Défreezer\" les couches du modèle de base\n",
    "for layer in base_model.layers[-15:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Construction du modèle (partie top)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation = 'relu')(x)\n",
    "x = Dropout(rate = 0.2)(x) # Couche de Dropout pour éviter l'overfitting\n",
    "x = Dense(512, activation = 'relu')(x)\n",
    "x = Dropout(rate = 0.2)(x) # Couche de Dropout pour éviter l'overfitting\n",
    "outputs = Dense(num_classes, activation='softmax')(x) # 4 classes de radiographies\n",
    "\n",
    "# Instanciation du modèle\n",
    "model = Model(inputs = base_model.input, outputs = outputs)\n",
    "\n",
    "\n",
    "\n",
    "## Compilation du modèle\n",
    "\n",
    "# Création fct de perte\n",
    "opt = tf.keras.optimizers.Adam(learning_rate= 0.001)\n",
    "\n",
    "# Compilation\n",
    "resnet_model.compile(loss = 'categorical_crossentropy',\n",
    "                     optimizer = opt,\n",
    "                     metrics = ['accuracy']\n",
    "                     )\n",
    "\n",
    "\n",
    "\n",
    "# Résumé du modèle\n",
    "model_summary = model.summary()\n",
    "print(model_summary)\n",
    "\n",
    "# Poids de classes (à adapter selon ton jeu de données ou préférences)\n",
    "# Exemple : favoriser la détection de la classe 0 et sous-pondérer la 3\n",
    "class_weights = {\n",
    "    0: 3.0,     # COVID\n",
    "    1: 1.0,     #Lung_Opacity\n",
    "    2: 1.0,     # Normal\n",
    "    3: 1.0      # Viral Pneumonia\n",
    "}\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "weight_code = ''\n",
    "for value in class_weights.values():\n",
    "    weight_code += str(int(value))  #mise en forme x111 pour l'enregistrement\n",
    "\n",
    "model_name = f'resnet50V2_{timestr}_{weight_code}'\n",
    "model_path=f'models/{model_name}'\n",
    "\n",
    "## Instanciation callbacks\n",
    "\n",
    "# a) Arrêt prématuré\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss',       # Métrique surveillée\n",
    "                               min_delta = 0.01,           # Changement minimum de la métrique surveillée\n",
    "                               patience = 5,               # Nombre d'epochs sans amélioration pour arrêt\n",
    "                               mode = 'min',               # L'entraînement s'arrête quand 'val_loss' cesse de décroître\n",
    "                               verbose = 1,                # Affichage de l'epoch d'arrêt\n",
    "                               restore_best_weights = True # Restauration des meilleurs poids (après arrêt)\n",
    "                              )\n",
    "\n",
    "\n",
    "## b) Learning_rate réduit\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor = 'val_loss',       # Métrique surveillée\n",
    "                                         min_delta = 0.01,           # Changement minimum de la métrique surveillée\n",
    "                                         patience = 3,               # Nombre d'epochs sans amélioration pour arrêt\n",
    "                                         factor = 0.1,               # Learning_rate divisé par 10 (multiplié par 0.1)\n",
    "                                         cooldown = 4,               # Pause de 4 epochs entre 2 cycles\n",
    "                                         verbose = 1,                # Affichage de l'epoch d'arrêt\n",
    "                                         restore_best_weights = True # Restauration des meilleurs poids (après arrêt)\n",
    "                                        )\n",
    "\n",
    "\n",
    "## c) Sauvegarde auto du modèle durant l'entraînement\n",
    "from keras import callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "save0 = ModelCheckpoint(f\"{model_name}.keras\",\n",
    "                       save_best_only=True, # Only saves when the model is considered the \"best\"\n",
    "                       monitor='val_loss', # The metric name to monitor\n",
    "                       mode='min' # Overwrite elder model when 'val_loss' is min\n",
    "                      )\n",
    "save1 = ModelCheckpoint(f\"{model_name}.h5\",\n",
    "                       save_best_only=True, # Only saves when the model is considered the \"best\"\n",
    "                       monitor='val_loss', # The metric name to monitor\n",
    "                       mode='min' # Overwrite elder model when 'val_loss' is min\n",
    "                      )\n",
    "\n",
    "\n",
    "resnet_history = model.fit(train_ds,\n",
    "                                  epochs=20,\n",
    "                                  validation_data=val_ds,\n",
    "                                  class_weight=class_weights,\n",
    "                                  callbacks = [early_stopping,\n",
    "                                               reduce_learning_rate,\n",
    "                                               save0, save1])\n",
    "###### On n'utilise pas de batch_size, car déjà inclus dans les générateurs ######\n",
    "\n",
    "## Extractions des valeurs de perte train/val\n",
    "train_loss = resnet_history.history[\"loss\"]\n",
    "val_loss = resnet_history.history[\"val_loss\"]\n",
    "\n",
    "train_mae = resnet_history.history[\"accuracy\"]\n",
    "val_mae = resnet_history.history[\"val_accuracy\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Affichage de la fonction de perte\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Tracer la perte MSE\n",
    "plt.subplot(121)\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('sparse_categorical_crossentropy par époque')\n",
    "plt.ylabel('sparse_categorical_crossentropy')\n",
    "plt.xlabel('Époque')\n",
    "plt.legend(['Entraînement', 'Validation'], loc='best')\n",
    "\n",
    "# Tracer l'erreur absolue moyenne (MAE)\n",
    "plt.subplot(122)\n",
    "plt.plot(train_mae)\n",
    "plt.plot(val_mae)\n",
    "plt.title('Accuracy par époque')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Époque')\n",
    "plt.legend(['Entraînement', 'Validation'], loc='best')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f2353",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librairie d'utilitaires\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, confusion_matrix, f1_score\n",
    "from collections import Counter\n",
    "from tensorflow.keras import Input\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import pickle\n",
    "# Définir les catégories et les paramètres\n",
    "categories = [\"COVID\", \"Normal\", \"Lung_Opacity\", \"Viral_Pneumonia\"] # Les quatres classes du jeu de données\n",
    "base_dir = ... # Chemain d'accès au dossiers des images\n",
    "# Paramètres\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "# Chargement des datasets\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    os.path.join(base_dir, \"train\"),\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    image_size=img_size,\n",
    "    seed=seed,\n",
    "    shuffle=True\n",
    ")\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    os.path.join(base_dir, \"val\"),\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    image_size=img_size,\n",
    "    seed=seed,\n",
    "    shuffle=False\n",
    ")\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    os.path.join(base_dir, \"test\"),\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    image_size=img_size,\n",
    "    seed=seed,\n",
    "    shuffle=False\n",
    ")\n",
    "#  Appliquer le prétraitement VGG19\n",
    "preprocess_input = tf.keras.applications.vgg19.preprocess_input\n",
    "train_ds = train_ds.map(lambda x, y: (preprocess_input(x), y)).prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.map(lambda x, y: (preprocess_input(x), y)).prefetch(AUTOTUNE)\n",
    "test_ds = test_ds.map(lambda x, y: (preprocess_input(x), y)).prefetch(AUTOTUNE)\n",
    "# Mapping des classes et ponderation, pour application de poids d'entraienment différent entre les classes\n",
    "class_mapping = {name: idx for idx, name in enumerate(categories)}\n",
    "# Poids à appliquer aux différentes classes\n",
    "class_weights = {\n",
    "    class_mapping['Viral_Pneumonia']: 1.0,\n",
    "    class_mapping['Lung_Opacity']: 1.0,\n",
    "    class_mapping['Normal']: 1.0,\n",
    "    class_mapping['COVID']: 1.0\n",
    "}\n",
    "# Création de la fonction du modèle avec VGG19\n",
    "def build_model(input_shape=(224, 224, 3), num_classes=len(categories)):\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=input_shape, pooling=\"max\")\n",
    "    # Gèle toutes les couches du modèle de base\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    # Déverrouille uniquement la couche 'block5_conv4'\n",
    "    for layer in base_model.layers:\n",
    "        if layer.name == 'block5_conv4':\n",
    "            layer.trainable = True\n",
    "            print(f\"Déverrouillage de : {layer.name}\")\n",
    "    x = base_model.output # permet la lecture des couches pour une analyse d'interprétabilité\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    # Recompile le modèle après modification des couches entraînables\n",
    "    model.compile(optimizer=Adamax(learning_rate=0.0001),  # Learning rate plus bas pour le fine-tuning\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "# Fonction des callback\n",
    "def get_callbacks():\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6, verbose=1)\n",
    "    checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    return [early_stop, reduce_lr, checkpoint, tensorboard]\n",
    "#  Entraînement du modèle\n",
    "model = build_model()\n",
    "callbacks = get_callbacks()\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7690f4df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bc27a42",
   "metadata": {},
   "source": [
    "# Interprétabilité Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], \n",
    "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "# Dé-unbatch et convertit en liste\n",
    "all_test_data = list(test_ds.unbatch().as_numpy_iterator())\n",
    "\n",
    "# Tire 100 échantillons au hasard (ou moins si moins de 100 dans le dataset)\n",
    "selected_samples = random.sample(all_test_data, k=min(100, len(all_test_data)))\n",
    "\n",
    "# Sépare images et labels\n",
    "test_images = [img for img, label in selected_samples]\n",
    "test_labels = [np.argmax(label) for img, label in selected_samples]\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Prédictions sur ces images\n",
    "pred_probs = model.predict(test_images)\n",
    "pred_labels = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "# Trouver 2 bonnes et 2 mauvaises\n",
    "correct_idx = np.where(pred_labels == test_labels)[0]\n",
    "incorrect_idx = np.where(pred_labels != test_labels)[0]\n",
    "\n",
    "\n",
    "selected_correct = np.random.choice(correct_idx, size=min(2, len(correct_idx)), replace=False)\n",
    "selected_incorrect = np.random.choice(incorrect_idx, size=min(2, len(incorrect_idx)), replace=False)\n",
    "# Fusion\n",
    "selected_idx = np.concatenate([selected_correct, selected_incorrect])\n",
    "\n",
    "# Grad-CAM et affichage\n",
    "last_conv_layer_name = 'conv5_block3_3_conv'  # pour ResNet50V2\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(10, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(selected_idx):\n",
    "    img_array = np.expand_dims(test_images[idx], axis=0)\n",
    "\n",
    "    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
    "\n",
    "    # Superposer heatmap à l’image\n",
    "    img = test_images[idx].astype('uint8')\n",
    "    heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap_colored = cv2.applyColorMap(np.uint8(255*heatmap_resized), cv2.COLORMAP_JET)\n",
    "    superimposed_img = cv2.addWeighted(cv2.cvtColor(img, cv2.COLOR_RGB2BGR), 0.6, heatmap_colored, 0.4, 0)\n",
    "\n",
    "    \n",
    "    ax = axes[2*i]\n",
    "    ax.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n",
    "    ax.set_title(f\"Réel: {class_names[test_labels[idx]]}, Prédit: {class_names[pred_labels[idx]]}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax = axes[2*i+1]\n",
    "    probs = pred_probs[idx]\n",
    "    classes = [f'Classe {i}' for i in range(len(probs))]\n",
    "    ax.bar(class_names, probs, color='skyblue')\n",
    "    ax.set_title(f'Probabilités pour l\\'image {idx}')\n",
    "    ax.set_ylabel('Probabilité')\n",
    "\n",
    "# if not os.path.exists(f\"{model_path}/gradcam\"):\n",
    "#     os.makedirs(f\"{model_path}/gradcam\")\n",
    "# timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "# plt.savefig(f\"{model_path}/gradcam/gradcam_{timestamp}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
