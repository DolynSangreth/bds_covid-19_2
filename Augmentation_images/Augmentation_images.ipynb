{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d4d940e",
   "metadata": {},
   "source": [
    "# Augmentation des images des classes minoritaires \n",
    "\n",
    "<div>\n",
    "<b>Objectif:</b>\n",
    " Nous avons remarqué un desequilibre de nos quatres classes avec deux minoritaire et deux majoritaire (ce referer a la Data Visualisation de notre jeu de donnée) ce qui induisait des problèmes dans l'entrainement des modèle et donc pour les étapes suivante (F1_score mediocre). L'augmentation d'image à donc été choisi comme outil pour palier au manque de données dans les classes minoritaire et pour réequilibrer les classes et donc le jeu de données\n",
    "</div>\n",
    "\n",
    "*Jeu de données* : [Accessible sur Kaggle](https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database)\n",
    "\n",
    "Après construction de la stratégie générale permettant de resoudre ce déséquilibre, plusieurs combinaisons de paramètres ont été tester afin de determiner celle permettant l'optention des meilleurs metriques (nottament accuracy et F1-score).\n",
    "\n",
    "A noté que la determination des valeurs des paramètres pour l'optimisation est arbitraire et est vouée a variée en fonction des autres étapes de preprocessing\n",
    "\n",
    "Pour determiner les scores des différentes preprocessing et donc la pertinance de ces derniers dans le cadre de notre jeu de donnée un modèle `Benchmark` à été développer. Il est important de noter que celui-ci va évoluée et est en aucun cas le modèle final.\n",
    "\n",
    "\n",
    "**Modèle Benchmark :**\n",
    "``` python\n",
    "\n",
    "# Charger MobileNetV2 sans la dernière couche (include_top=False)\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "base_model.trainable = False  # On ne touche pas aux poids de MobileNet\n",
    "\n",
    "# Ajouter notre classifieur (4 classes ici)\n",
    "inputs = Input(shape=(256, 256, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(4, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data=val_dataset,\n",
    "                    epochs=5)\n",
    "```\n",
    "Celui ci à été appliquer à toutes les combinaisons de paramètres comme indiquer dans le tableau suivant :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0126330",
   "metadata": {},
   "source": [
    "| ID | Nombre d'occurances | Nombre de couches |Valeur en argument |Bitch size |\n",
    "|:--------:|:--------:|:--------:|:--------:|:--------:|\n",
    "|1|4000 |3|0.1|32|\n",
    "|2|5000|3|0.1|32|\n",
    "|**3**|**6000**|**3**|**0.1**|**32** |\n",
    "|4|6000|2|0.1|32|\n",
    "|5|6000|3 |0.1|32|\n",
    "|6|6000|4|0.1|32|\n",
    "|7|6000|3|0.1|32|\n",
    "|8|6000|3|0.2|32|\n",
    "|9|6000|3|0.3|32|\n",
    "|10|6000|3|0.1|16|\n",
    "|11|6000|3|0.1|32|\n",
    "|12|6000|3|0.1|64|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d256d37",
   "metadata": {},
   "source": [
    "<div>\n",
    "<b>Table 1:</b>\n",
    "Combinaison de paramètres tester pour leur optimisation\n",
    "</div>\n",
    "\n",
    "> La combinaison de paramètre selectionnée correspond à l'`ID 3`, avec un accuracy et un F1-score de 0.92. \n",
    "\n",
    "<div>\n",
    "<b>Pour allez plus loin:</b>\n",
    "Le test de l'optimisation du modèle benchmark a aussi été réalisé sur les images avec masques en suivant le protocole optimisé et les valeurs de accuracy et de F1-score étaient de 0.89\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb3d55",
   "metadata": {},
   "source": [
    "# Augmentation d'image après optimisation des paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6803b132",
   "metadata": {},
   "source": [
    "## Chargement des packages :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7bfe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importation des données \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "\n",
    "#Augmentation des images \n",
    "import random\n",
    "from tensorflow.keras.layers import RandomZoom, RandomRotation, RandomContrast, Rescaling, Resizing, RandomBrightness\n",
    "\n",
    "#Modelisation\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras import Input\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e21e21",
   "metadata": {},
   "source": [
    "## Chargement des différentes classes en local à partir de dossiers : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc94be",
   "metadata": {},
   "source": [
    "### Pour la classe Normale : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "078a6e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10192 images chargées avec la forme (10192, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# Chargement des images Normal\n",
    "folder_path = r\"C:\\Users\\grego\\OneDrive\\Bureau\\Data_Science\\Projet\\COVID-19_Radiography_Dataset\\COVID-19_Radiography_Dataset\\Normal\\images\"\n",
    "target_size = (256, 256)\n",
    "\n",
    "# Charger et convertir les images en tableau numpy\n",
    "images_normal_np = np.array([\n",
    "    img_to_array(load_img(os.path.join(folder_path, f), target_size=target_size))\n",
    "    for f in os.listdir(folder_path)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "])\n",
    "\n",
    "print(f\"{len(images_normal_np)} images chargées avec la forme {images_normal_np.shape}\")\n",
    "\n",
    "# Créer un Dataset TensorFlow avec les images et les labels (0 pour Normal)\n",
    "dataset_normal = tf.data.Dataset.from_tensor_slices(\n",
    "    (images_normal_np, tf.zeros(len(images_normal_np), dtype=tf.int32))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed2cf9",
   "metadata": {},
   "source": [
    "### Pour la classe Covid : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a66e7f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3616 images chargées avec la forme (3616, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# Chargement des images COVID\n",
    "folder_path = r\"C:\\Users\\grego\\OneDrive\\Bureau\\Data_Science\\Projet\\COVID-19_Radiography_Dataset\\COVID-19_Radiography_Dataset\\COVID\\images\"\n",
    "target_size = (256, 256)\n",
    "\n",
    "# Charger et convertir les images en tableau numpy\n",
    "images_covid_np = np.array([\n",
    "    img_to_array(load_img(os.path.join(folder_path, f), target_size=target_size))\n",
    "    for f in os.listdir(folder_path)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "])\n",
    "\n",
    "print(f\"{len(images_covid_np)} images chargées avec la forme {images_covid_np.shape}\")\n",
    "\n",
    "# Créer un Dataset TensorFlow avec les images et les labels (1 pour COVID)\n",
    "dataset_covid = tf.data.Dataset.from_tensor_slices(\n",
    "    (images_covid_np, tf.ones(len(images_covid_np), dtype=tf.int32))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c0e7c",
   "metadata": {},
   "source": [
    "### Pour la classe Lung Opacity : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4215797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012 images chargées avec la forme (6012, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# Chargement des images Lung Opacity\n",
    "folder_path = r\"C:\\Users\\grego\\OneDrive\\Bureau\\Data_Science\\Projet\\COVID-19_Radiography_Dataset\\COVID-19_Radiography_Dataset\\Lung_opacity\\images\"\n",
    "target_size = (256, 256)\n",
    "\n",
    "# Charger et convertir les images en tableau numpy\n",
    "images_opacity_np = np.array([\n",
    "    img_to_array(load_img(os.path.join(folder_path, f), target_size=target_size))\n",
    "    for f in os.listdir(folder_path)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "])\n",
    "\n",
    "print(f\"{len(images_opacity_np)} images chargées avec la forme {images_opacity_np.shape}\")\n",
    "\n",
    "# Créer un Dataset TensorFlow avec les images et les labels (2 pour Lung_opacity)\n",
    "dataset_opacity = tf.data.Dataset.from_tensor_slices(\n",
    "    (images_opacity_np, tf.fill(len(images_opacity_np), tf.constant(2, dtype=tf.int32))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fcdb74",
   "metadata": {},
   "source": [
    "### Pour la classe Viral Pneumonia : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f421144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1345 images chargées avec la forme (1345, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# Chargement des images Viral Pneumonia\n",
    "folder_path = r\"C:\\Users\\grego\\OneDrive\\Bureau\\Data_Science\\Projet\\COVID-19_Radiography_Dataset\\COVID-19_Radiography_Dataset\\Viral_Pneumonia\\images\"\n",
    "target_size = (256, 256)\n",
    "\n",
    "# Charger et convertir les images en tableau numpy\n",
    "images_pneumonia_np = np.array([\n",
    "    img_to_array(load_img(os.path.join(folder_path, f), target_size=target_size))\n",
    "    for f in os.listdir(folder_path)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "])\n",
    "\n",
    "print(f\"{len(images_pneumonia_np)} images chargées avec la forme {images_pneumonia_np.shape}\")\n",
    "\n",
    "# Créer un Dataset TensorFlow avec les images et les labels (3 pour Viral Pneumonia)\n",
    "dataset_pneumonia = tf.data.Dataset.from_tensor_slices(\n",
    "    (images_pneumonia_np, tf.fill(len(images_pneumonia_np), tf.constant(2, dtype=tf.int32))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a31aeb",
   "metadata": {},
   "source": [
    "## Preprocessing/Augmentation d'images : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d355ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe 1 : 6000 images\n",
      "Classe 3 : 6000 images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ⚙️ Paramètres\n",
    "target_per_class = 6000\n",
    "random.seed(42)\n",
    "\n",
    "# 🔧 Couches d'augmentation\n",
    "random_zoom = RandomZoom(0.1)\n",
    "random_rotation = RandomRotation(0.1)\n",
    "random_contrast = RandomContrast(0.1)\n",
    "rescale = Rescaling(1./255)\n",
    "resize = Resizing(256, 256)  # ⬅️ Adapté à MobileNetV2\n",
    "\n",
    "augmentations = [random_zoom, random_rotation, random_contrast, resize, rescale]\n",
    "\n",
    "# 🔄 Fonction d'augmentation\n",
    "def augment_image(image, augmentations):\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    for aug_fn in augmentations:\n",
    "        image = aug_fn(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "# 📦 Augmentation par classe\n",
    "def augmenter_dataset_monoclasse(dataset, label, target, augmentations):\n",
    "    dataset_list = list(dataset)\n",
    "    images = [img for img, _ in dataset_list]\n",
    "    labels = [label] * len(images)\n",
    "\n",
    "    count = len(images)\n",
    "    print(f\"Classe {label} : {count} images\")\n",
    "\n",
    "    if count > target:\n",
    "        print(f\" ✂️ Réduction à {target} images pour la classe {label}\")\n",
    "        images = images[:target]\n",
    "        labels = [label] * target\n",
    "\n",
    "    if count < target:\n",
    "        to_generate = target - count\n",
    "        print(f\" ➕ Génération de {to_generate} images pour la classe {label}\")\n",
    "        for _ in range(to_generate):\n",
    "            img = images[random.randint(0, count - 1)]\n",
    "            aug_img = augment_image(img, augmentations)\n",
    "            images.append(aug_img)\n",
    "            labels.append(label)\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "\n",
    "# 🧪 Appliquer l’augmentation individuellement\n",
    "dataset_covid     = augmenter_dataset_monoclasse(dataset_covid,     label=1, target=target_per_class, augmentations=augmentations)\n",
    "dataset_pneumonia = augmenter_dataset_monoclasse(dataset_pneumonia, label=3, target=target_per_class, augmentations=augmentations)\n",
    "dataset_opacity = dataset_opacity.take(6000)\n",
    "dataset_normal = dataset_normal.take(6000)\n",
    "\n",
    "# ✅ Dataset complet équilibré\n",
    "full_dataset = dataset_covid.concatenate(dataset_pneumonia)\\\n",
    "                               .concatenate(dataset_normal)\\\n",
    "                               .concatenate(dataset_opacity)\n",
    "\n",
    "full_dataset_list = list(full_dataset)\n",
    "images, labels = zip(*full_dataset_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87260296",
   "metadata": {},
   "source": [
    "## Mise en forme pré-entrainement : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23043da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Labels du batch train :  [1 3 0 3 3 2 1 2 3 0 1 1 0 0 1 1 0 2 0 1 2 1 2 3 2 3 0 1 2 0 1 2]\n",
      "✅ Labels du batch val :  [3 1 3 1 3 1 0 2 0 3 1 3 2 3 0 2 1 1 1 3 0 1 3 0 2 3 2 0 0 0 3 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assurer que labels sont des ints hashables\n",
    "clean_labels = [int(label.numpy()) if isinstance(label, tf.Tensor) else int(label) for label in labels]\n",
    "\n",
    "# Split STRATIFIÉ (permet d'équilibrée train et validation)\n",
    "images_by_class = defaultdict(list)\n",
    "for img, label in zip(images, clean_labels):\n",
    "    images_by_class[label].append(img)\n",
    "\n",
    "train_images, train_labels = [], []\n",
    "val_images, val_labels = [], []\n",
    "\n",
    "for label, imgs in images_by_class.items():\n",
    "    split_idx = int(0.8 * len(imgs))\n",
    "    train_images += imgs[:split_idx]\n",
    "    train_labels += [label] * split_idx\n",
    "    val_images += imgs[split_idx:]\n",
    "    val_labels += [label] * (len(imgs) - split_idx)\n",
    "\n",
    "# Mélange avec sécurité\n",
    "combined_train = list(zip(train_images, train_labels))\n",
    "combined_val = list(zip(val_images, val_labels))\n",
    "random.shuffle(combined_train)\n",
    "random.shuffle(combined_val)\n",
    "\n",
    "if combined_train:\n",
    "    train_images, train_labels = zip(*combined_train)\n",
    "else:\n",
    "    train_images, train_labels = [], []\n",
    "\n",
    "if combined_val:\n",
    "    val_images, val_labels = zip(*combined_val)\n",
    "else:\n",
    "    val_images, val_labels = [], []\n",
    "\n",
    "# Générateurs robustes\n",
    "def generator_train():\n",
    "    for img, label in zip(train_images, train_labels):\n",
    "        yield tf.convert_to_tensor(img, dtype=tf.float32), tf.convert_to_tensor(label, dtype=tf.int32)\n",
    "\n",
    "def generator_val():\n",
    "    for img, label in zip(val_images, val_labels):\n",
    "        yield tf.convert_to_tensor(img, dtype=tf.float32), tf.convert_to_tensor(label, dtype=tf.int32)\n",
    "\n",
    "# Définition de l'output signature pour correspondres a nos attentes \n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(256, 256, 3), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    ")\n",
    "\n",
    "# Construction des datasets\n",
    "train_dataset = tf.data.Dataset.from_generator(generator_train, output_signature=output_signature)\n",
    "val_dataset = tf.data.Dataset.from_generator(generator_val, output_signature=output_signature)\n",
    "\n",
    "BATCH_SIZE = 32 # paramètre optimiser \n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Vérification de la stratification\n",
    "for _, labels_batch in train_dataset.take(1):\n",
    "    print(\"✅ Labels du batch train : \", labels_batch.numpy())\n",
    "for _, labels_batch in val_dataset.take(1):\n",
    "    print(\"✅ Labels du batch val : \", labels_batch.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f9b42",
   "metadata": {},
   "source": [
    "## Creation et instance du modèle benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d2c2b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "WARNING:tensorflow:From c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger MobileNetV2 sans la dernière couche (include_top=False)\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "base_model.trainable = False  # On ne touche pas aux poids de MobileNet\n",
    "\n",
    "# Ajouter notre classifieur (4 classes ici)\n",
    "inputs = Input(shape=(256, 256, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(4, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703786d",
   "metadata": {},
   "source": [
    "## Entrainement du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02e7f3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "600/600 [==============================] - 332s 545ms/step - loss: 0.6945 - accuracy: 0.7102 - val_loss: 0.3792 - val_accuracy: 0.8467\n",
      "Epoch 2/5\n",
      "600/600 [==============================] - 308s 514ms/step - loss: 0.5303 - accuracy: 0.7859 - val_loss: 0.3353 - val_accuracy: 0.8679\n",
      "Epoch 3/5\n",
      "453/600 [=====================>........] - ETA: 59s - loss: 0.4991 - accuracy: 0.8026Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_9816\\556007479.py\", line 5, in <module>\n",
      "    history = model.fit(train_dataset,\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 1418, in get_graph_debug_info\n",
      "    graph_debug_info.ParseFromString(proto_data)\n",
      "google.protobuf.message.DecodeError: Error parsing message\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\executing\\executing.py\", line 317, in executing\n",
      "    args = executing_cache[key]\n",
      "KeyError: (<code object get_graph_debug_info at 0x0000014FD893FA80, file \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 1402>, 1442447620736, 80)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1160, in get_records\n",
      "    res = list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\stack_data\\core.py\", line 565, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\stack_data\\utils.py\", line 84, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\stack_data\\core.py\", line 555, in mapper\n",
      "    return cls(f, options)\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\stack_data\\core.py\", line 520, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\executing\\executing.py\", line 369, in executing\n",
      "    args = find(source=cls.for_frame(frame), retry_cache=True)\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\executing\\executing.py\", line 252, in for_frame\n",
      "    return cls.for_filename(frame.f_code.co_filename, frame.f_globals or {}, use_cache)\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\executing\\executing.py\", line 270, in for_filename\n",
      "    result = source_cache[filename] = cls._for_filename_and_lines(filename, lines)\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\executing\\executing.py\", line 281, in _for_filename_and_lines\n",
      "    result = source_cache[(filename, lines)] = cls(filename, lines)\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\stack_data\\core.py\", line 81, in __init__\n",
      "    self.asttokens()\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\executing\\executing.py\", line 413, in asttokens\n",
      "    return ASTTokens(\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\asttokens\\asttokens.py\", line 65, in __init__\n",
      "    self.mark_tokens(self._tree)\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\asttokens\\asttokens.py\", line 76, in mark_tokens\n",
      "    MarkTokens(self).visit_tree(root_node)\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\asttokens\\mark_tokens.py\", line 49, in visit_tree\n",
      "    util.visit_tree(node, self._visit_before_children, self._visit_after_children)\n",
      "  File \"c:\\Users\\grego\\anaconda3\\envs\\projet_Covid19\\lib\\site-packages\\asttokens\\util.py\", line 194, in visit_tree\n",
      "    done.add(current)\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data=val_dataset,\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578479f8",
   "metadata": {},
   "source": [
    "# Metriques : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for batch_x, batch_y in val_dataset:\n",
    "    preds = model.predict(batch_x)\n",
    "    y_pred.extend(np.argmax(preds, axis=1))  # prédictions argmax\n",
    "    y_true.extend(batch_y.numpy())  # convertir les labels en numpy\n",
    "\n",
    "# Optionnel : cast en array\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Rapport\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Prédit')\n",
    "plt.ylabel('Réel')\n",
    "plt.title('Matrice de confusion')\n",
    "plt.show()\n",
    "\n",
    "# F1-scores\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"F1 macro     : {f1_macro:.4f}\")\n",
    "print(f\"F1 weighted  : {f1_weighted:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet_Covid19",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
